---
---

@string{aps = {American Physical Society,}}

@article{xu2024auroralong,
  abbr={preprint},
  title={Bringing RNNs Back to Efficient Open-Ended Video Understanding},
  author={Xu, Weili and Song, Enxin and Chai, Wenhao and Wen, Xuexiang, and Ye, Tian and Wang, Gaoang},
  publisher={preprint},
  month={11},
  year={2024},
  additional_info={*In submission to CVPR 2025*},
  selected={true},
  pdf={9135.pdf},
  supp={CVPR_2025_AuroraLong_Supplementary.pdf},
  abstract={The challenge of long video understanding lies in its high computational complexity and prohibitive memory cost, since the memory and computation required by transformer-based LLMs scale quadratically with input sequence length. We proposed AuroraLong to address this challenge by replacing the LLM component in MLLMs with RWKV, an RNN-like language model that handles input sequence of arbitrary length with constant-size hidden states. To further increase throughput and efficiency, as well as to reduce the gap between RWKV's 4k context length and the long video token sequence length, we combine visual token merge with linear RNN models by reordering the visual tokens by their sizes in ascending order. AuroraLong shows superior performance on various video benchmarks, for example, obtaining an average accuracy of 87.0 on scene transition in MVBench, beating GPT-4V (83.5) and Gemini Pro (75.4), highliting the possibilities that efficient linear RNNs can democratize long video understanding. To our best knowledge, we are the first to use a non-transformer LLM backbone for video understanding.},
  preview={AuroraLong-preview.png},
  bibtex_show={true}
}